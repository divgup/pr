{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef load_cifar_batch(pathname,i):\n    with open(pathname,'rb') as f:\n        datadict = pickle.load(f,encoding='bytes') \n        #print(datadict)\n        batch_labels = datadict[b'labels']\n        batch_data = datadict[b'data']\n        batch_data =batch_data.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n        if i==1:\n            #print(batch_data.shape)\n            plt.imshow(batch_data[0].astype('uint8'))\n        batch_labels = np.array(batch_labels)\n    return batch_data,batch_labels      \ndef load_cifar10():\n    x_data = []\n    y_data = []\n    for i in range(1,6):\n        path=os.path.join('/kaggle/input/cifar10/cifar-10-batches-py','data_batch_{}'.format(i))\n        batch_x,batch_y = load_cifar_batch(path,i)\n        x_data.append(batch_x)\n        y_data.append(batch_y)\n    Xtr= np.concatenate(x_data) \n    Ytr= np.concatenate(y_data)\n    Xte,Yte = load_cifar_batch(os.path.join('/kaggle/input/cifar10/cifar-10-batches-py','test_batch',),i)\n    return Xtr,Ytr,Xte,Yte","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain,Ytrain,Xtest,Ytest=load_cifar10()\nprint(Xtrain.shape)\nprint(Ytrain.shape)\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nnum_classes = len(classes)\nsamples_per_class = 7\nfor y, cls in enumerate(classes):\n    idxs = np.flatnonzero(Ytrain == y)\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + y + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        plt.imshow(Xtrain[idx].astype('uint8'))\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()\n#print(Xte.shape)\n#print(Yte.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain_reshaped = Xtrain.reshape(-1,32*32*3)\nXtest_reshaped = Xtest.reshape(-1,32*32*3)\nnum_train = 49000\nnum_val = 1000\nnum_test=1000\nnum_dev = 500\n\nmask= range(num_train,num_train+num_val)\nX_val = Xtrain_reshaped[mask]    \nY_val=Ytrain[mask]\n\nmask = range(num_train)\nX_train = Xtrain_reshaped[mask]\nY_train = Ytrain[mask]\n\nmask = np.random.choice(num_train, num_dev, replace=False)\nX_dev = X_train[mask]\nY_dev = Y_train[mask]\n\nmask = range(num_test)\nX_test = Xtest_reshaped[mask]\ny_test = Ytest[mask]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef process_data(X_train,X_test,X_val,X_dev):\n    mean_train = np.mean(Xtrain_reshaped,axis=0)\n    mean_test = np.mean(Xtest_reshaped,axis=0)\n    print(mean_train[:10]) # print a few of the elements\n    plt.figure(figsize=(4,4))\n    plt.imshow(mean_train.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n    plt.show()\n    X_train-=mean_train.astype('uint8')\n    X_val-=mean_train.astype('uint8')\n    X_test-=mean_train.astype('uint8')\n    X_dev-=mean_train.astype('uint8')\n    #print(X_train.dtype)\n    #X_train=X_train//255\n    #X_val//=255\n    #X_test//=255\n    #X_dev//=255\n    #X_train/=255\n    #X_test/=255\n    return X_train,X_val,X_test,X_dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,X_test,X_dev = process_data(X_train,X_test,X_val,X_dev)\nX_train = np.hstack([X_train,np.ones((X_train.shape[0],1))])\nX_val = np.hstack([X_val,np.ones((X_val.shape[0],1))])\nX_dev = np.hstack([X_dev,np.ones((X_dev.shape[0],1))])\nX_test = np.hstack([X_test,np.ones((X_test.shape[0],1))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorized_svm_loss(X,W,y,reg):\n    delta=1\n    dW = np.zeros(W.shape)\n    num_train = X.shape[0]\n    num_class = W.shape[1]\n    out = np.dot(X,W)\n    margins = out - out[np.arange(num_train),y].T[:,None]+delta\n    margins[margins<0]=0\n    margins[np.arange(num_train),y] = 0\n    loss=margins\n    loss = np.sum(loss)\n    margins[margins>0]=1\n    rowsum = np.sum(margins,axis=1)\n    margins[np.arange(num_train),y] = -rowsum\n    dW = np.dot(X.T,margins)\n    dW/=num_train\n    loss/=num_train\n    loss+=0.5*reg*np.sum(W*W)\n    dW+=reg*W\n    return dW,loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Once you've implemented the gradient, recompute it with the code below\n# and gradient check it with the function we provided for you\nimport random\n# Compute the loss and its gradient at W.\ngrad,loss = vectorized_svm_loss(X_dev,W, Y_dev, 0.0)\n\n# Numerically compute the gradient along several randomly chosen dimensions, and\n# compare them with your analytically computed gradient. The numbers should match\n# almost exactly along all dimensions.\n#def grad_check_sparse():\ndef grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n  \"\"\"\n  sample a few random elements and only return numerical\n  in this dimensions.\n  \"\"\"\n\n  for i in range(num_checks):\n    ix = tuple([random.randrange(m) for m in x.shape])\n\n    oldval = x[ix]\n    x[ix] = oldval + h # increment by h\n    fxph = f(x) # evaluate f(x + h)\n    x[ix] = oldval - h # increment by h\n    fxmh = f(x) # evaluate f(x - h)\n    x[ix] = oldval # reset\n\n    grad_numerical = (fxph - fxmh) / (2 * h)\n    grad_analytic = analytic_grad[ix]\n    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n    \nf = lambda w: vectorized_svm_loss(X_dev,w, Y_dev, 0.0)[1]\ngrad_numerical = grad_check_sparse(f, W, grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nnum_class = 10\nprint(X_train.shape)\nW = np.random.randn(X_train.shape[1],num_class)*0.0001\ntic = time.time()\nreg = 1e-5\nxbatch = X_train[0:100]\nybatch = Y_train[0:100]\ngrad,loss = vectorized_svm_loss(xbatch,W,ybatch,reg)\ntoc = time.time()\nprint(toc-tic)\nprint(grad,loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X_train,X_val,W):\n    train_pred = np.argmax(np.dot(X_train,W),axis=1)  \n    val_pred = np.argmax(np.dot(X_val,W),axis=1)  \n    return train_pred,val_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\ndef train(X_train,Y_train,X_val,Y_val,W,eta,reg):\n    num_train = X_train.shape[0]\n    num_val = X_val.shape[0]\n    trainloss_array = []\n    valloss_array = []\n    #reg = 1e+03\n    \n    tot_loss=0\n    batch_size=100\n    for i in range(num_train//batch_size):\n        batch = X_train[i*batch_size:(i+1)*batch_size]\n        label = Y_train[i*batch_size:(i+1)*batch_size]\n        grad,loss = vectorized_svm_loss(batch,W,label,reg)\n        #print(loss)\n        #print(grad)\n        W-=eta*grad\n        tot_loss+=loss\n    train_loss=tot_loss/(num_train//batch_size)\n    #print(train_loss)\n    tot_loss=0    \n    for i in range(num_val//batch_size):\n        val_batch = X_val[i*batch_size:(i+1)*batch_size]\n        val_label = Y_val[i*batch_size:(i+1)*batch_size]\n        grad,valloss = vectorized_svm_loss(val_batch,W,val_label,reg)    \n        tot_loss+=valloss\n    val_loss = tot_loss/(num_val//batch_size)\n    return train_loss,val_loss,W","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\nval_loss = []\netas=[]\nepochs = 80\neta=0.0000001\nnum = 30\n\nlearning_rates = 10 ** np.random.uniform(-7, -6, size=num)\nregs = 10 ** np.random.uniform(3.5, 4.5, size=num)\nt_acc = []\nv_acc = []\nfor lr,reg in zip(learning_rates,regs):\n    W = np.random.randn(X_train.shape[1],num_class)*0.001\n    for epoch in range(epochs):\n        X_train,Y_train = shuffle(X_train,Y_train)\n        trloss,valloss,W = train(X_train,Y_train,X_val,Y_val,W,lr,reg)\n    trpred,vlpred = predict(X_train,X_val,W)    \n    train_acc = np.mean(trpred==Y_train)\n    val_acc = np.mean(vlpred==Y_val)\n    t_acc.append(train_acc)\n    v_acc.append(val_acc)\n    train_loss.append(trloss)    \n    val_loss.append(valloss)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(train_loss)\nplt.show()\nplt.figure()\nplt.plot(val_loss)\nplt.figure()\nplt.plot(v_acc)\nplt.figure()\nplt.plot(t_acc)\nprint(val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred_index = np.argmax(np.dot(X_test[0],W))\n#correct_label = y_test[0]\npositive=0\n\npositive=0\nfor i in range(X_test.shape[0]):\n    correct_label = y_test[i]\n    pred_index = np.argmax(np.dot(X_test[i],W))\n    if(pred_index==correct_label):\n        positive+=1\ntest_acc = positive/X_test.shape[0]\nprint('train_acc: {}'.format(train_acc*100),'test_acc: {}'.format(test_acc*100),'val_acc: {}'.format(val_acc*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(learning_rates[17],regs[17])\nlr,reg = learning_rates[17],regs[17]\nW = np.random.randn(X_train.shape[1],num_class)*0.001\nfor epoch in range(epochs):\n    X_train,Y_train = shuffle(X_train,Y_train)\n    trloss,valloss,W = train(X_train,Y_train,X_val,Y_val,W,lr,reg)\n#    trpred,vlpred = predict(X_train,X_val,W)    \n#    train_acc = np.mean(trpred==Y_train)\n#    val_acc = np.mean(vlpred==Y_val)\n#    t_acc.append(train_acc)\n#    v_acc.append(val_acc)\nprint(trloss)\nprint(valloss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = W[:-1,:]\nw = w.reshape(32,32,3,10)\nnp.savez('weights.npz',name1=W,name2=w)\nminw,maxw = np.ndarray.min(w),np.ndarray.max(w)\nprint(minw,maxw)\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    tempw =255.0*(w[:,:,:,i].squeeze()-minw)/(maxw-minw)\n    #print(tempw)\n    plt.imshow(tempw.astype('uint8'))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trpred,vlpred = predict(X_train,X_val,W)    \ntrain_acc = np.mean(trpred==Y_train)\nval_acc = np.mean(vlpred==Y_val)\nprint(\"val_acc:{}\".format(val_acc),\"train_acc:{}\".format(train_acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}